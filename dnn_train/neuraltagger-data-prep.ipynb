{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ha', 'a')"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_PATH='/home/denocris/Desktop/NeuralTagger/dnn_train'\n",
    "\n",
    "# select the ambiguity\n",
    "AMB='ha_a'\n",
    "\n",
    "# POS & PLACEHOLDER in NT-v1-POS is automatically done in the main tf code\n",
    "# If you are using NT-v1 set SUB_PLACEHOLDER to True\n",
    "POS_DATASET = False\n",
    "SUB_PLACEHOLDER  = True\n",
    "\n",
    "# version of the dataset\n",
    "# nph: non-placeholder (it is added directly in the main tf code)\n",
    "VERSION='v48.tsv'\n",
    "\n",
    "\n",
    "case = AMB.split('_')\n",
    "\n",
    "amb1, amb2 = case[0], case[1]\n",
    "\n",
    "amb1, amb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Class Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class1: 397233, Class2: 395916, Ratio: 0.997\n"
     ]
    }
   ],
   "source": [
    "#! wc -l /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e/ver/*\n",
    "class1_num = !wc -l ./amb_raw/$AMB/$amb1/* | tail -n 1\n",
    "#! wc -l /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e/cong/*\n",
    "class2_num = !wc -l ./amb_raw/$AMB/$amb2/* | tail -n 1\n",
    "\n",
    "class1_num = class1_num[0].strip().split()\n",
    "class2_num = class2_num[0].strip().split()\n",
    "\n",
    "ratio = int(class1_num[0]) / float(class2_num[0]) if int(class1_num[0]) < int(class2_num[0]) else int(class2_num[0]) / float(class1_num[0])\n",
    "\n",
    "print('Class1: %d, Class2: %d, Ratio: %.3f' %(int(class1_num[0]), int(class2_num[0]), ratio)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute ratio for future class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9966845654817198"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = int(class1_num[0]) / float(class2_num[0]) if int(class1_num[0]) < int(class2_num[0]) else int(class2_num[0]) / float(class1_num[0])\n",
    "\n",
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverting ambiguities\n",
    "\n",
    "The largest class must be the 2nd one (for the code to work properly). SO if class 1 is larger, than invert them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted --> ('a', 'ha')\n"
     ]
    }
   ],
   "source": [
    "if int(class1_num[0]) >= int(class2_num[0]):\n",
    "    amb2, amb1 = case[0], case[1]   \n",
    "    print('Inverted -->', (amb1, amb2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ad-hoc Sentences\n",
    "\n",
    "Sentences generated by grammars or hand-written "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./adhocsents/ha_a/a_adhoc.txt\n",
      "./adhocsents/ha_a/ha_adhoc_byhand.txt\n",
      "./adhocsents/ha_a/a_adhoc_byhand.txt\n",
      "./adhocsents/ha_a/chiama_a.txt\n",
      "./adhocsents/ha_a/ha_adhoc.txt\n",
      "Ex of an dhoc sentence from ambiguity a: traducimi questo testo da italiano a portoghese\n",
      "Ex of an dhoc sentence from ambiguity ha: che età ha kycgw\n"
     ]
    }
   ],
   "source": [
    "class1_adhoc = []\n",
    "class2_adhoc  = []\n",
    "\n",
    "for name in glob.glob(r'./adhocsents/'+AMB+'/*'):\n",
    "    print(name)\n",
    "    file = open(name, \"r\")\n",
    "    if name.split('/')[-1].split('_')[0]==amb1:\n",
    "        adhoc1_tmp = [ line.rstrip().lower() for line in file ]\n",
    "        class1_adhoc += adhoc1_tmp\n",
    "    elif name.split('/')[-1].split('_')[0]==amb2:\n",
    "        adhoc2_tmp = [ line.rstrip().lower() for line in file ]\n",
    "        class2_adhoc += adhoc2_tmp\n",
    "        \n",
    "adhoc_dict = {amb1: class1_adhoc, amb2: class2_adhoc}\n",
    "\n",
    "print('Ex of an dhoc sentence from ambiguity %s: %s' %(amb1, np.random.choice(class1_adhoc).strip()))\n",
    "print('Ex of an dhoc sentence from ambiguity %s: %s' %(amb2, np.random.choice(class2_adhoc).strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Data-Frame (Balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory_1, directory_2):\n",
    "    # NOTE: Put in directory_2 the largest corpus\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"class\"] = []\n",
    "    l1 = 0\n",
    "    \n",
    "    class1num = ! cat $directory_1/* | wc -l\n",
    "    class2num = ! cat $directory_2/* | wc -l\n",
    "\n",
    "    assert int(class2num[0]) >= int(class1num[0])\n",
    "    \n",
    "    #Append ad-hoc sentences\n",
    "    for s in adhoc_dict[amb1]:\n",
    "        s = s.strip()\n",
    "        data[\"sentence\"].append(s)\n",
    "        data[\"class\"].append(1)\n",
    "        \n",
    "        \n",
    "    for s in adhoc_dict[amb2]:\n",
    "        s = s.strip()\n",
    "        data[\"sentence\"].append(s)\n",
    "        data[\"class\"].append(0)\n",
    "        \n",
    "    for file_path in os.listdir(directory_1):\n",
    "        with tf.gfile.GFile(os.path.join(directory_1 , file_path), \"rb\") as f:\n",
    "                # strip() removes white spaces before and after the string\n",
    "                # decode() converst a byte object ('b) in a python3 string\n",
    "                list_of_sentences = [s.strip().decode() for s in f.readlines()]\n",
    "                num_rows_1 = len(list_of_sentences)\n",
    "                for i in range(num_rows_1):\n",
    "                    data[\"sentence\"].append(list_of_sentences[i])\n",
    "                    data[\"class\"].append(1)\n",
    "    \n",
    "    for file_path in os.listdir(directory_2):\n",
    "        with tf.gfile.GFile(os.path.join(directory_2 , file_path), \"rb\") as f:\n",
    "                # strip() removes white spaces before and after the string\n",
    "                # decode() converst a byte object ('b) in a python3 string\n",
    "                list_of_sentences = [s.strip().decode() for s in f.readlines() if np.random.random() <= ratio]\n",
    "                num_rows_1 = len(list_of_sentences)\n",
    "                for i in range(num_rows_1):\n",
    "                    data[\"sentence\"].append(list_of_sentences[i])\n",
    "                    data[\"class\"].append(0)\n",
    "\n",
    "    return pd.DataFrame.from_dict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.3 s, sys: 336 ms, total: 9.64 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "directory_1 = ROOT_PATH + '/amb_raw/'+AMB+'/'+amb1+'/'\n",
    "directory_2 = ROOT_PATH + '/amb_raw/'+AMB+'/'+amb2+'/'\n",
    "\n",
    "# NOTE: Put in directory_2 the largest class\n",
    "dataset_df = load_dataset(directory_1, directory_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If classes are not perfectly balaced, do not worry! We added non balanced adhoc sentences...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>398342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>404063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence\n",
       "class          \n",
       "0        398342\n",
       "1        404063"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Balanced\n",
    "print('If classes are not perfectly balaced, do not worry! We added non balanced adhoc sentences...')\n",
    "dataset_df.groupby('class').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bixby ristoranti a udine\n",
      "quanta ram ha il mio telefono\n",
      "centri commerciali a civitavecchia\n",
      "che sintomi ha l epatite\n",
      "eventi a budapest\n",
      "quanta ram ha il mio computer\n",
      "sciopero a sydney\n",
      "quanta memoria ha il mio computer\n",
      "pioggia a l aquila\n",
      "quanta ram ha il mio cellulare\n",
      "bixby aperitivo a l aquila\n",
      "quanta memoria ha il mio cellulare\n"
     ]
    }
   ],
   "source": [
    "# Print some samples\n",
    "for i in range(6):\n",
    "    print(dataset_df.iloc[i]['sentence'])\n",
    "    print(dataset_df.iloc[-i -1]['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove puntuations and spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude1 = ['\\t', '\"', '?', '!','-', '_'] # list\n",
    "exclude2 = [\"'\", \"  \", \"   \", \"    \", \"     \"] # list\n",
    "\n",
    "def clean_text(text):\n",
    "    for c in exclude1:\n",
    "        text=text.replace(c,'')\n",
    "    for c in exclude2:\n",
    "        text=text.replace(c, \" \")\n",
    "        \n",
    "    text = re.sub(r'\\bbè\\b', 'è', text)\n",
    "    text = re.sub(r'\\bèb\\b', 'è', text)\n",
    "    text = re.sub(r'\\bbdove\\b', 'dove', text)\n",
    "    text = re.sub(r'\\bbcome\\b', 'come', text)\n",
    "    text = re.sub(r'\\bbquale\\b', 'quale', text)\n",
    "    text = re.sub(r'\\bbquando\\b', 'quando', text)\n",
    "    text = re.sub(r'\\bbquanto\\b', 'quanto', text)\n",
    "    text = re.sub(r'ã', 'à', text)\n",
    "\n",
    "    return text.lower().strip()\n",
    "\n",
    "sentence_processed = list(map(lambda x: clean_text(x), dataset_df['sentence'].values))\n",
    "\n",
    "dataset_df['sentence'] = sentence_processed\n",
    "\n",
    "#dataset_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HO_O only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off = .5 #percentage to remove\n",
    "\n",
    "if AMB=='ho_o':\n",
    "\n",
    "    # create a column which entries are random numbers between 0 and 1\n",
    "    dataset_df['random'] = [np.random.random() for i in range(len(dataset_df))] \n",
    "\n",
    "    # select rows whose sentence do NOT starts with 'o '\n",
    "    df_notstartswith_o = dataset_df[~(dataset_df[\"sentence\"].str.startswith('o '))]\n",
    "\n",
    "    # select 50% of rows whose sentence starts with 'o '\n",
    "    df_startswith_o_50 = dataset_df[((dataset_df[\"sentence\"].str.startswith('o ')) & (dataset_df[\"random\"] <= cut_off))]\n",
    "    \n",
    "    # concat the two dataframe\n",
    "    dataset_df = pd.concat([df_notstartswith_o, df_startswith_o_50], ignore_index=True)\n",
    "    # remove 'random' column\n",
    "    dataset_df = dataset_df.drop(columns=['random'])\n",
    "\n",
    "\n",
    "    print(len(df_notstartswith_o), len(df_startswith_o_50), len(dataset_df))\n",
    "    assert len(df_notstartswith_o)+len(df_startswith_o_50)==len(dataset_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hanno_anno only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off = .75 #percentage to remove\n",
    "\n",
    "if AMB=='hanno_anno':\n",
    "\n",
    "    # create a column which entries are random numbers between 0 and 1\n",
    "    dataset_df['random'] = [np.random.random() for i in range(len(dataset_df))] \n",
    "\n",
    "    # select rows whose sentence do NOT starts with 'o '\n",
    "    df_notcontains_chehanno = dataset_df[~(dataset_df[\"sentence\"].str.contains(\"che hanno\"))]\n",
    "\n",
    "    # select 50% of rows whose sentence starts with 'o '\n",
    "    df_contains_chehanno = dataset_df[((dataset_df[\"sentence\"].str.contains(\"che hanno\")) & (dataset_df[\"random\"] <= cut_off))]\n",
    "    \n",
    "    # concat the two dataframe\n",
    "    dataset_df = pd.concat([df_notcontains_chehanno, df_contains_chehanno], ignore_index=True)\n",
    "    # remove 'random' column\n",
    "    dataset_df = dataset_df.drop(columns=['random'])\n",
    "\n",
    "\n",
    "    print(len(df_notcontains_chehanno), len(df_contains_chehanno), len(dataset_df))\n",
    "    assert len(df_notcontains_chehanno)+len(df_contains_chehanno)==len(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cosa è ti ha preso per scemo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a me piacciono ie tue riviste sporcaccione</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>come namor personaggio della marvel comics ha ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>spero che tu capisca che potresti esserci util...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>non hai nessuna possibilità contro loki fino a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>ha cambiato idea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>che tempo fa lunedì a il cairo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>henry e io vi abbiamo cercato più a lungo ma a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>o fanne a meno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>e a quel punto sono stata meglio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                           sentence\n",
       "0      0                       cosa è ti ha preso per scemo\n",
       "1      1         a me piacciono ie tue riviste sporcaccione\n",
       "2      0  come namor personaggio della marvel comics ha ...\n",
       "3      1  spero che tu capisca che potresti esserci util...\n",
       "4      1  non hai nessuna possibilità contro loki fino a...\n",
       "5      0                                   ha cambiato idea\n",
       "6      1                     che tempo fa lunedì a il cairo\n",
       "7      1  henry e io vi abbiamo cercato più a lungo ma a...\n",
       "8      1                                     o fanne a meno\n",
       "9      1                   e a quel punto sono stata meglio"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = [i for i in range(dataset_df.shape[0])]\n",
    "random.shuffle(index)\n",
    "dataset_df = dataset_df.set_index([index]).sort_index()\n",
    "\n",
    "dataset_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples:  802405\n"
     ]
    }
   ],
   "source": [
    "tot_sample = len(dataset_df)\n",
    "print('Total samples: ', tot_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = len(dataset_df) #400000\n",
    "\n",
    "dataset_red = dataset_df.head(num_sample)\n",
    "\n",
    "dataset_pos = dataset_red.copy()\n",
    "\n",
    "\n",
    "assert id(dataset_red) != id(dataset_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics about the dataset\n",
    "\n",
    "This cell is just for statistics. It is not mandatory to run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class       1.000000\n",
       "sentence    7.628967\n",
       "dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting number of words and mean\n",
    "dataset_red.astype('str').applymap(lambda x: str(x).count(' ') + 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class        1\n",
       "sentence    13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length sentence\n",
    "dataset_red.astype('str').applymap(lambda x: str(x).count(' ') + 1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class       0.000000\n",
       "sentence    2.823671\n",
       "dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length sentence\n",
    "dataset_red.astype('str').applymap(lambda x: str(x).count(' ') + 1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of the lengths\n",
    "%matplotlib inline\n",
    "\n",
    "length_sentence = dataset_red.astype('str').applymap(lambda x: str(x).count(' ') + 1)\n",
    "plt.hist(length_sentence['sentence'],bins=range(50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitute ambiguities with a placeholder (tannutuva)\n",
    "\n",
    "it works if SUB_PLACEHOLDER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A placeholder has been inserted: it is tannutuva\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cosa è ti tannutuva preso per scemo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tannutuva me piacciono ie tue riviste sporcacc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>come namor personaggio della marvel comics tan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>spero che tu capisca che potresti esserci util...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>non hai nessuna possibilità contro loki fino t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>tannutuva cambiato idea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>che tempo fa lunedì tannutuva il cairo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>henry e io vi abbiamo cercato più tannutuva lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>o fanne tannutuva meno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>e tannutuva quel punto sono stata meglio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                           sentence\n",
       "0      0                cosa è ti tannutuva preso per scemo\n",
       "1      1  tannutuva me piacciono ie tue riviste sporcacc...\n",
       "2      0  come namor personaggio della marvel comics tan...\n",
       "3      1  spero che tu capisca che potresti esserci util...\n",
       "4      1  non hai nessuna possibilità contro loki fino t...\n",
       "5      0                            tannutuva cambiato idea\n",
       "6      1             che tempo fa lunedì tannutuva il cairo\n",
       "7      1  henry e io vi abbiamo cercato più tannutuva lu...\n",
       "8      1                             o fanne tannutuva meno\n",
       "9      1           e tannutuva quel punto sono stata meglio"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toSubstitute = case # list\n",
    "\n",
    "# def substitute_amniguity_old(text, placeholder):\n",
    "#     for c in toSubstitute:\n",
    "#         text=text.replace(c,placeholder)\n",
    "#     return text.lower().strip()\n",
    "\n",
    "def substitute_amniguity(text, placeholder):\n",
    "    for c in toSubstitute:\n",
    "        text=re.sub(r\"\\b%s\\b\" %(c),placeholder, text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "placeholder = 'tannutuva'\n",
    "res = 'A placeholder has NOT been inserted! This is fine if your main tf code does it automatically.'\n",
    "if SUB_PLACEHOLDER:\n",
    "    res = 'A placeholder has been inserted: it is '+placeholder\n",
    "    sentence_processed = list(map(lambda text: substitute_amniguity(text,placeholder), dataset_red['sentence'].values))\n",
    "    dataset_red['sentence'] = sentence_processed\n",
    "\n",
    "print(res)\n",
    "dataset_red.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split for Tagger Classifier (Train, Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Set size: 722164\n",
      "Validation-Set size: 80241\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splitter =  model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.1,random_state=19850610)\n",
    "\n",
    "splits = list(splitter.split(X=dataset_red['sentence'], y=dataset_red['class']))\n",
    "train_index = splits[0][0]\n",
    "valid_index = splits[0][1]\n",
    "\n",
    "train_df = dataset_red.loc[train_index,:]\n",
    "print('Training-Set size: %d' %len(train_df))\n",
    "\n",
    "valid_df = dataset_red.loc[valid_index,:]\n",
    "print('Validation-Set size: %d' %len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40407"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_value_counts = valid_df['class'].value_counts()\n",
    "validation_value_counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "1    363656\n",
      "0    358508\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 49.64\n",
      "class 1 %: 50.36\n",
      "\n",
      "Validation Set\n",
      "1    40407\n",
      "0    39834\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 49.64\n",
      "class 1 %: 50.36\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "training_value_counts = train_df['class'].value_counts()\n",
    "print(training_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(training_value_counts[0]/len(train_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(training_value_counts[1]/len(train_df)*100,2)))\n",
    "print(\"\")\n",
    "print(\"Validation Set\")\n",
    "validation_value_counts = valid_df['class'].value_counts()\n",
    "print(validation_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(validation_value_counts[0]/len(valid_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(validation_value_counts[1]/len(valid_df)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION='nph_100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train is saved:  datasets/ha_a/train_data_v48.tsv\n",
      "valid is saved:  datasets/ha_a/valid_data_v48.tsv\n"
     ]
    }
   ],
   "source": [
    "train_df.to_csv(os.path.join(ROOT_PATH, 'datasets/'+AMB+'/train_data_'+VERSION), header=False, index=False, sep='\\t')\n",
    "valid_df.to_csv(os.path.join(ROOT_PATH, 'datasets/'+AMB+'/valid_data_'+VERSION), header=False, index=False, sep='\\t')\n",
    "\n",
    "print('train is saved: ', 'datasets/'+AMB+'/train_data_'+VERSION)\n",
    "print('valid is saved: ', 'datasets/'+AMB+'/valid_data_'+VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Vocabulary and Save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cosa è ti tannutuva preso per scemo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tannutuva me piacciono ie tue riviste sporcacc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>come namor personaggio della marvel comics tan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>spero che tu capisca che potresti esserci util...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>non hai nessuna possibilità contro loki fino t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                           sentence\n",
       "0      0                cosa è ti tannutuva preso per scemo\n",
       "1      1  tannutuva me piacciono ie tue riviste sporcacc...\n",
       "2      0  come namor personaggio della marvel comics tan...\n",
       "3      1  spero che tu capisca che potresti esserci util...\n",
       "4      1  non hai nessuna possibilità contro loki fino t..."
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not POS_DATASET:\n",
    "    dataset_pos = dataset_red\n",
    "    \n",
    "dataset_red.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# def get_vocab(df):\n",
    "#     vocab = set()\n",
    "#     for text in df['sentence'].values:\n",
    "#         words = text.split(' ')\n",
    "#         # remove digits\n",
    "#         words_only = [w for w in words if not w.isdigit()]\n",
    "#         # exclude words shorter than 2, but not numbers. exclude words with numbers inside, i.e. '3cris', 'c45ris', 'cris23'\n",
    "#         words_ = [w for w in words_only if len(w) > 0 ]\n",
    "#         word_set = set(words_)\n",
    "#         vocab.update(word_set)\n",
    "    \n",
    "#     return list(vocab)\n",
    "\n",
    "def get_all_words(df):\n",
    "    allWords = [text.split(' ') for text in df['sentence'].values]\n",
    "    allWords = list(chain(*allWords))\n",
    "    allWords = [w for w in allWords if not w.isdigit()]\n",
    "    allWords = [w for w in allWords if len(w) > 0 ]\n",
    "    return allWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 6564461\n",
      "CPU times: user 12.8 s, sys: 1.12 s, total: 13.9 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vocab = get_vocab(dataset_pos)\n",
    "# print('--------------------', len(vocab))\n",
    "\n",
    "allWords = get_all_words(dataset_pos)\n",
    "print('--------------------', len(allWords))\n",
    "\n",
    "vocab = set(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt_allWords = Counter(allWords)\n",
    "\n",
    "vocab_words_sorted_by_appearence = sorted(cnt_allWords.items(), key=lambda kv: len(vocab) - kv[1])\n",
    "#vocab_words_sorted_by_appearence\n",
    "\n",
    "vocab_words_sorted_by_appearence_list = [word[0] for word in vocab_words_sorted_by_appearence]\n",
    "#vocab_words_sorted_by_appearence_list, len(vocab_words_sorted_by_appearence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial vocabulary of  142399  has been reduced to the most frequent  15000  words\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = False\n",
    "REDUCED_SIZE_VOC = True\n",
    "SIZE_VOC = 15000\n",
    "\n",
    "vocab = vocab_words_sorted_by_appearence_list\n",
    "\n",
    "\n",
    "if STOP_WORDS:\n",
    "    stop_words = get_stop_words('italian') \n",
    "    vocab = [w for w in vocab if w not in stop_words]\n",
    "    words_and_frequence = [ (word, freq) for (word, freq) in vocab_words_sorted_by_appearence if word not in stop_words]\n",
    "\n",
    "init_len = len(vocab)\n",
    "if REDUCED_SIZE_VOC:\n",
    "    vocab = vocab[0:SIZE_VOC]\n",
    "    \n",
    "print(\"The initial vocabulary of \", init_len, \" has been reduced to the most frequent \", len(vocab), \" words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'propn' in vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding to the vocabulary relevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the number of words is:  15000\n"
     ]
    }
   ],
   "source": [
    "# PROPN must be added to the vocabulary. Now is not present, but in the main code lots of proper names are going \n",
    "# to be pos-tagged with PROPN\n",
    "\n",
    "#adhoc_words = ['propn','tannutuva']\n",
    "adhoc_words = ['tannutuva']\n",
    "\n",
    "for word in adhoc_words:\n",
    "    if word in vocab:\n",
    "        continue\n",
    "    else:\n",
    "        vocab.append(word)\n",
    "    \n",
    "vocab = set(vocab)    \n",
    "print(\"Now the number of words is: \", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove from the vocabulary the ambiguities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not SUB_PLACEHOLDER:\n",
    "    vocab.remove(amb1)\n",
    "    vocab.remove(amb2)\n",
    "\n",
    "amb1 in vocab, amb2 in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'propn' in vocab, 'tannutuva' in vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizevoc = str(int(SIZE_VOC/1000))+'k'\n",
    "\n",
    "PATH_VOCAB = 'datasets/'+AMB+'/vocab_'+sizevoc+'_'+VERSION\n",
    "tmp_PATH_VOCAB = re.sub(\"vocab\",'tmp_vocab', PATH_VOCAB )\n",
    "PATH_NWORDS = re.sub(\"vocab\",'n_words', PATH_VOCAB )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/ha_a/tmp_vocab_15k_v48.tsv'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_PATH_VOCAB \n",
    "#! ls datasets/sarà_sara/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH ='./'# +tmp_PATH_VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/ha_a/tmp_vocab_15k_v48.tsv'"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_PATH_VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vacab is saved:  datasets/ha_a/vocab_15k_v48.tsv\n"
     ]
    }
   ],
   "source": [
    "PAD_WORD = '#=KS=#'\n",
    "\n",
    "PATH_VOC = os.path.join(ROOT_PATH, tmp_PATH_VOCAB)\n",
    "with open(PATH_VOC , 'w') as file:\n",
    "    file.write(\"{}\\n\".format(PAD_WORD))\n",
    "    for word in vocab:\n",
    "        file.write(\"{}\\n\".format(word))\n",
    "\n",
    "\n",
    "! awk '! /^[0-9]+$/' ./$tmp_PATH_VOCAB |  sed '/^$/d'  > ./$PATH_VOCAB\n",
    "#! rm -rf ./$tmp_PATH_VOCAB\n",
    "\n",
    "print('vacab is saved: ', PATH_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: datasets/ha_a/vocab_15k_v48.tsv: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!$PATH_VOCAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the exact number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15001\n",
      "n_words is saved:  ./datasets/ha_a/n_words_15k_v48.tsv\n"
     ]
    }
   ],
   "source": [
    "#! sed 's/[^a-zA-Z]//g' /home/asr/prj_SlotTagger/dnn_train/datasets/è_e/vocab_15k_swin_v13_tmp.tsv | awk '! /^[0-9]+$/' | sed '/^$/d' > /home/asr/prj_SlotTagger/dnn_train/datasets/è_e/vocab_15k_swin_v13.tsv \n",
    "nwords=! wc -l ./$PATH_VOCAB\n",
    "nwords=nwords[0].strip().split()[0]\n",
    "print(nwords)\n",
    "\n",
    "PATH_WORDS = os.path.join(ROOT_PATH, PATH_NWORDS)        \n",
    "with open(PATH_WORDS, 'w') as file:\n",
    "#with open('/home/asr/Data/classif_task/jsgf_data/n_words.tsv', 'w') as file:\n",
    "    file.write(str(nwords))\n",
    "    \n",
    "print('n_words is saved: ', PATH_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gawk: inplace:14: warning: inplace_begin: Cannot stat `valid_data_v12.tsv' (No such file or directory)\n",
      "gawk: inplace:14: fatal: cannot open file `valid_data_v12.tsv' for reading (No such file or directory)\n",
      "gawk: inplace:14: warning: inplace_begin: Cannot stat `train_data_v12.tsv' (No such file or directory)\n",
      "gawk: inplace:14: fatal: cannot open file `train_data_v12.tsv' for reading (No such file or directory)\n"
     ]
    }
   ],
   "source": [
    "!gawk -i inplace -F '\\t' '{ print $2 \"\\t\" $1}' valid_data_v12.tsv\n",
    "!gawk -i inplace -F '\\t' '{ print $2 \"\\t\" $1}' train_data_v12.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
