{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asr/tensorflow-cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/asr/tensorflow-cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH='/home/asr/prj_SlotTagger/dnn_train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Six Ambiguity Problem: VERB or NUM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus_six_num: 105181, Corpus_six_ver: 285704, Ratio: 0.368\n",
      "CPU times: user 98.5 ms, sys: 4.64 ms, total: 103 ms\n",
      "Wall time: 101 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "num_lines_num  = sum(1 for line in open(ROOT_PATH + '/amb_raw/sei_6/num/sei_num'))\n",
    "num_lines_ver  = sum(1 for line in open(ROOT_PATH + '/amb_raw/sei_6/ver/sei_verb'))\n",
    "\n",
    "ratio = num_lines_num  / float(num_lines_ver) \n",
    "print('Corpus_six_num: %d, Corpus_six_ver: %d, Ratio: %0.3f' %(num_lines_num, num_lines_ver, ratio)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Data-Frame (Balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory_1, directory_2):\n",
    "    # NOTE: Put in directory_2 the largest corpus\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"class\"] = []\n",
    "    l1 = 0\n",
    "    for file_path in os.listdir(directory_1):\n",
    "        with tf.gfile.GFile(os.path.join(directory_1 , file_path), \"rb\") as f:\n",
    "                # strip() removes white spaces before and after the string\n",
    "                # decode() converst a byte object ('b) in a python3 string\n",
    "                list_of_sentences = [s.strip().decode() for s in f.readlines()]\n",
    "                num_rows_1 = len(list_of_sentences)\n",
    "                for i in range(num_rows_1):\n",
    "                    data[\"sentence\"].append(list_of_sentences[i])\n",
    "                    data[\"class\"].append(1)\n",
    "    \n",
    "    for file_path in os.listdir(directory_2):\n",
    "        with tf.gfile.GFile(os.path.join(directory_2 , file_path), \"rb\") as f:\n",
    "                # strip() removes white spaces before and after the string\n",
    "                # decode() converst a byte object ('b) in a python3 string\n",
    "                list_of_sentences = [s.strip().decode() for s in f.readlines() if np.random.random() <= ratio]\n",
    "                num_rows_1 = len(list_of_sentences)\n",
    "                for i in range(num_rows_1):\n",
    "                    data[\"sentence\"].append(list_of_sentences[i])\n",
    "                    data[\"class\"].append(0)\n",
    "\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 38.9 ms, total: 1.19 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "directory_1 = ROOT_PATH + '/amb_raw/sei_6/num/'\n",
    "directory_2 = ROOT_PATH + '/amb_raw/sei_6/ver/'\n",
    "\n",
    "dataset_df = load_dataset(directory_1, directory_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence\n",
       "class          \n",
       "0        104815\n",
       "1        105181"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Balanced\n",
    "dataset_df.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a al comma dopo le parole inferiore a sei\n",
      "whoa whoa whoa sei cifre\n",
      "a anni accoltellò all'addome jimmy davis un bambino di sei anni e lo nascose nei\n",
      "warner bros ha comprato il mio libro per sei cifre\n",
      "a anni da del mondo e con l'uscita al primo turno da sei slam\n",
      "vuoi tirare avanti con una somma di appena sei cifre per così tanto\n",
      "a anni e sei mesi dalla\n",
      "tre lettere sei cifre\n",
      "a anni passa nelle giovanili del milan dove trascorre sei stagioni laureandosi per due volte\n",
      "sei denunce per furto d auto in due delle quali si dichiara\n",
      "a a sei\n",
      "sei cifre per cominciare\n",
      "a a sei minuti dal\n",
      "sei cifre non mi serviva altro\n",
      "aa vv sei secoli di musica nel duomo\n",
      "sei cifre facili\n",
      "a balaustra e sei gruppi di scalini che su ambedue i lati conducono verso la città\n",
      "sei cifre ecco qua\n",
      "a baltimora nel maryland il più giovane di sei figli\n",
      "sei cifre come il codice a barre che potrebbe essere l algoritmo magico\n"
     ]
    }
   ],
   "source": [
    "# Print some samples\n",
    "for i in range(10):\n",
    "    print(dataset_df.iloc[i]['sentence'])\n",
    "    print(dataset_df.iloc[-i -1]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a al comma dopo le parole inferiore a sei</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a anni accoltellò all'addome jimmy davis un ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a anni da del mondo e con l'uscita al primo tu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a anni e sei mesi dalla</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a anni passa nelle giovanili del milan dove tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  class\n",
       "0          a al comma dopo le parole inferiore a sei      1\n",
       "1  a anni accoltellò all'addome jimmy davis un ba...      1\n",
       "2  a anni da del mondo e con l'uscita al primo tu...      1\n",
       "3                            a anni e sei mesi dalla      1\n",
       "4  a anni passa nelle giovanili del milan dove tr...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209991</th>\n",
       "      <td>sei denunce per furto d auto in due delle qual...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209992</th>\n",
       "      <td>tre lettere sei cifre</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209993</th>\n",
       "      <td>vuoi tirare avanti con una somma di appena sei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209994</th>\n",
       "      <td>warner bros ha comprato il mio libro per sei c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209995</th>\n",
       "      <td>whoa whoa whoa sei cifre</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  class\n",
       "209991  sei denunce per furto d auto in due delle qual...      0\n",
       "209992                              tre lettere sei cifre      0\n",
       "209993  vuoi tirare avanti con una somma di appena sei...      0\n",
       "209994  warner bros ha comprato il mio libro per sei c...      0\n",
       "209995                           whoa whoa whoa sei cifre      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    8.523429\n",
       "class       1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting number of words and mean\n",
    "dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    286\n",
       "class         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length sentence\n",
    "dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    4.187989\n",
       "class       0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length sentence\n",
    "dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of the lengths\n",
    "%matplotlib inline\n",
    "\n",
    "length_sentence = dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1)\n",
    "plt.hist(length_sentence['sentence'],bins=range(50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sia in patria da sei anni</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mononucleosi tornò ad allenarsi dopo due mesi ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tu sei stato dichiarato non colpevole per infe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sposato con un figlio di sei anni</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>okay niente gite per almeno sei mesi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>per provare che sei più forte di lui</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>che eravamo insieme alle sei del mattino il qu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>voglio aggiungere attività a un calendario per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tu vieni con noi sei ancora in punizione</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>il modo in cui sei riuscito a girarla</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  class\n",
       "0                          sia in patria da sei anni      1\n",
       "1  mononucleosi tornò ad allenarsi dopo due mesi ...      1\n",
       "2  tu sei stato dichiarato non colpevole per infe...      0\n",
       "3                  sposato con un figlio di sei anni      1\n",
       "4               okay niente gite per almeno sei mesi      1\n",
       "5               per provare che sei più forte di lui      0\n",
       "6  che eravamo insieme alle sei del mattino il qu...      1\n",
       "7  voglio aggiungere attività a un calendario per...      1\n",
       "8           tu vieni con noi sei ancora in punizione      0\n",
       "9              il modo in cui sei riuscito a girarla      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "index = [i for i in range(dataset_df.shape[0])]\n",
    "random.shuffle(index)\n",
    "dataset = dataset_df.set_index([index]).sort_index()\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sia in patria da sei anni</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mononucleosi tornò ad allenarsi dopo due mesi ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tu sei stato dichiarato non colpevole per infe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sposato con un figlio di sei anni</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>okay niente gite per almeno sei mesi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>per provare che sei più forte di lui</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>che eravamo insieme alle sei del mattino il qu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>voglio aggiungere attività a un calendario per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tu vieni con noi sei ancora in punizione</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>il modo in cui sei riuscito a girarla</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ti sei sdebitato ormai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ho qualche impegno il sei luglio prossimo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ma sono solo sei settimane</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>se continui per altri sei isolati arriverai a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ti va di vederci dove abbiamo nuotato oggi all...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>e poi te ne sei andata senza una parola</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>e sei milioni per il miglioramento</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sei milioni a insonorizzarlo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>potresti pubblicarmi fotografie e video fatti ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>significa che sei bravo a letto</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence  class\n",
       "0                           sia in patria da sei anni      1\n",
       "1   mononucleosi tornò ad allenarsi dopo due mesi ...      1\n",
       "2   tu sei stato dichiarato non colpevole per infe...      0\n",
       "3                   sposato con un figlio di sei anni      1\n",
       "4                okay niente gite per almeno sei mesi      1\n",
       "5                per provare che sei più forte di lui      0\n",
       "6   che eravamo insieme alle sei del mattino il qu...      1\n",
       "7   voglio aggiungere attività a un calendario per...      1\n",
       "8            tu vieni con noi sei ancora in punizione      0\n",
       "9               il modo in cui sei riuscito a girarla      0\n",
       "10                             ti sei sdebitato ormai      0\n",
       "11          ho qualche impegno il sei luglio prossimo      1\n",
       "12                         ma sono solo sei settimane      1\n",
       "13  se continui per altri sei isolati arriverai a ...      1\n",
       "14  ti va di vederci dove abbiamo nuotato oggi all...      1\n",
       "15            e poi te ne sei andata senza una parola      0\n",
       "16                 e sei milioni per il miglioramento      1\n",
       "17                       sei milioni a insonorizzarlo      1\n",
       "18  potresti pubblicarmi fotografie e video fatti ...      1\n",
       "19                    significa che sei bravo a letto      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude1 = ['\\t', '\"', '?'] # list\n",
    "exclude2 = [\"'\", \"  \", \"   \", \"    \", \"     \"] # list\n",
    "\n",
    "def clean_text(text):\n",
    "    for c in exclude1:\n",
    "        text=text.replace(c,'')\n",
    "    for c in exclude2:\n",
    "        text=text.replace(c, \" \")\n",
    "    return text.lower().strip()\n",
    "\n",
    "sentence_processed = list(map(lambda text: clean_text(text), dataset['sentence'].values))\n",
    "\n",
    "dataset['sentence'] = sentence_processed\n",
    "\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quando ti sei dato al fumo\n",
      "sei un duro amico\n",
      "sei ore in ufficio vorrei vedere te\n",
      "prima settimana di guerra e te ne sei già beccata una nella gamba\n",
      "ricorda che siamo sei ore avanti\n",
      "sei un ingegnere lo sai\n",
      "sei venuto a trovare montecristo\n",
      "mi restavano sei mesi\n",
      "ti sei lamentato che non ero d aiuto e questo sta per cambiare\n",
      "sei venuto ad ammirare la tua opera\n",
      "non sono stata senza sesso per due settimana da quando sono stata in coma per sei\n",
      "terza in un girone a sei squadre\n",
      "tu sei divenuta un essere superiore\n",
      "coniugi ebbero altri sei figli\n",
      "visto che sei un auto rotta prima o poi ti fermerai giusto\n",
      "sembra che tu voglia dire qualcosa ma ti sei preso una bella mazzata\n",
      "pur essendo in forze di uno a sei nadir shah riscì a schiacciare l esercito mughal in\n",
      "vuol dire che sei ancora nella zona\n",
      "com è che sei stata tanto sciocca da venire qua sotto\n",
      "già l omicidio è avvenuto sei settimane fa\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(dataset.iloc[i]['sentence'])\n",
    "    print(dataset.iloc[-i -1]['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split for Semi-Supervised Dataset Learning (Train, Valid & Infer Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Set size: 157764\n",
      "Test-Set size: 52589\n"
     ]
    }
   ],
   "source": [
    "splitter = model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=19850610)\n",
    "\n",
    "splits = list(splitter.split(X=dataset['sentence'], y=dataset['class']))\n",
    "main_index = splits[0][0]\n",
    "test_index = splits[0][1]\n",
    "\n",
    "main_df = dataset.loc[main_index,:]\n",
    "print('Training-Set size: %d' %len(main_df))\n",
    "\n",
    "test_df = dataset.loc[test_index,:]\n",
    "print('Test-Set size: %d' %len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157764"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = main_df.dropna()\n",
    "len(main_df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52589"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df.dropna()\n",
    "len(test_df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Set size: 4732\n",
      "Inference-Set size: 153032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asr/tensorflow-cpu/lib/python3.6/site-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "splitter =  model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.97, random_state=19850610)\n",
    "\n",
    "splits = list(splitter.split(X=main_df['sentence'], y=main_df['class']))\n",
    "train_index = splits[0][0]\n",
    "infer_index = splits[0][1]\n",
    "\n",
    "train_df = main_df.loc[train_index,:]\n",
    "print('Training-Set size: %d' %len(train_df))\n",
    "\n",
    "infer_df = main_df.loc[infer_index,:]\n",
    "print('Inference-Set size: %d' %len(infer_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3574"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.dropna()\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114793"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_df = infer_df.dropna()\n",
    "len(infer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "1.0    1792\n",
      "0.0    1782\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 49.86\n",
      "class 1 %: 50.14\n",
      "\n",
      "Test Set\n",
      "1    26296\n",
      "0    26293\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 50.0\n",
      "class 1 %: 50.0\n",
      "\n",
      "Inference Set\n",
      "1.0    57604\n",
      "0.0    57189\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 49.82\n",
      "class 1 %: 50.18\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "training_value_counts = train_df['class'].value_counts()\n",
    "print(training_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(training_value_counts[0]/len(train_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(training_value_counts[1]/len(train_df)*100,2)))\n",
    "print(\"\")\n",
    "print(\"Test Set\")\n",
    "validation_value_counts = test_df['class'].value_counts()\n",
    "print(validation_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(validation_value_counts[0]/len(test_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(validation_value_counts[1]/len(test_df)*100,2)))\n",
    "print(\"\")\n",
    "print(\"Inference Set\")\n",
    "inference_value_counts = infer_df['class'].value_counts()\n",
    "print(inference_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(inference_value_counts[0]/len(infer_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(inference_value_counts[1]/len(infer_df)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['class'] = train_df['class'].apply(lambda x: str(int(x)))\n",
    "test_df['class'] = test_df['class'].apply(lambda x: str(int(x)))\n",
    "infer_df['class'] = infer_df['class'].apply(lambda x: str(int(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('/home/asr/rd_ssDataLearning/dataset/six/train_sei_sslear.tsv', header=False, index=False, sep='\\t')\n",
    "test_df.to_csv('/home/asr/rd_ssDataLearning/dataset/six/test_sei_sslear.tsv', header=False, index=False, sep='\\t')\n",
    "infer_df.to_csv('/home/asr/rd_ssDataLearning/dataset/six/infer_sei_sslear.tsv', header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split for Tagger Classifier (Train, Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Set size: 178496\n",
      "Validation-Set size: 31500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splitter =  model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=19850610)\n",
    "\n",
    "splits = list(splitter.split(X=dataset['sentence'], y=dataset['class']))\n",
    "train_index = splits[0][0]\n",
    "valid_index = splits[0][1]\n",
    "\n",
    "train_df = dataset.loc[train_index,:]\n",
    "print('Training-Set size: %d' %len(train_df))\n",
    "\n",
    "valid_df = dataset.loc[valid_index,:]\n",
    "print('Validation-Set size: %d' %len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "1    89404\n",
      "0    89092\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 49.91\n",
      "class 1 %: 50.09\n",
      "\n",
      "Validation Set\n",
      "1    15777\n",
      "0    15723\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 49.91\n",
      "class 1 %: 50.09\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "training_value_counts = train_df['class'].value_counts()\n",
    "print(training_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(training_value_counts[0]/len(train_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(training_value_counts[1]/len(train_df)*100,2)))\n",
    "print(\"\")\n",
    "print(\"Validation Set\")\n",
    "validation_value_counts = valid_df['class'].value_counts()\n",
    "print(validation_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(validation_value_counts[0]/len(valid_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(validation_value_counts[1]/len(valid_df)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df.to_csv(os.path.join(ROOT_PATH, 'datasets/sei_6/train_data_v2.tsv'), header=False, index=False, sep='\\t')\n",
    "valid_df.to_csv(os.path.join(ROOT_PATH, 'datasets/sei_6/valid_data_v2.tsv'), header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Vocabulary and Save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('italian') + get_stop_words('english')\n",
    "\n",
    "#my_stop_words = ['puoi','posso','vediamo','guarda','vorrei','voglio','dici','fammi']\n",
    "my_stop_words = []\n",
    "for my_word in my_stop_words:\n",
    "    stop_words.append(my_word)\n",
    "    \n",
    "# Important step for this dataset!!!!\n",
    "stop_words.remove('sei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'abbia',\n",
       " 'abbiamo',\n",
       " 'abbiano',\n",
       " 'abbiate',\n",
       " 'ad',\n",
       " 'adesso',\n",
       " 'agl',\n",
       " 'agli',\n",
       " 'ai',\n",
       " 'al',\n",
       " 'all',\n",
       " 'alla',\n",
       " 'alle',\n",
       " 'allo',\n",
       " 'allora',\n",
       " 'altre',\n",
       " 'altri',\n",
       " 'altro',\n",
       " 'anche',\n",
       " 'ancora',\n",
       " 'avemmo',\n",
       " 'avendo',\n",
       " 'avere',\n",
       " 'avesse',\n",
       " 'avessero',\n",
       " 'avessi',\n",
       " 'avessimo',\n",
       " 'aveste',\n",
       " 'avesti',\n",
       " 'avete',\n",
       " 'aveva',\n",
       " 'avevamo',\n",
       " 'avevano',\n",
       " 'avevate',\n",
       " 'avevi',\n",
       " 'avevo',\n",
       " 'avrai',\n",
       " 'avranno',\n",
       " 'avrebbe',\n",
       " 'avrebbero',\n",
       " 'avrei',\n",
       " 'avremmo',\n",
       " 'avremo',\n",
       " 'avreste',\n",
       " 'avresti',\n",
       " 'avrete',\n",
       " 'avrà',\n",
       " 'avrò',\n",
       " 'avuta',\n",
       " 'avute',\n",
       " 'avuti',\n",
       " 'avuto',\n",
       " 'c',\n",
       " 'che',\n",
       " 'chi',\n",
       " 'ci',\n",
       " 'coi',\n",
       " 'col',\n",
       " 'come',\n",
       " 'con',\n",
       " 'contro',\n",
       " 'cui',\n",
       " 'da',\n",
       " 'dagl',\n",
       " 'dagli',\n",
       " 'dai',\n",
       " 'dal',\n",
       " 'dall',\n",
       " 'dalla',\n",
       " 'dalle',\n",
       " 'dallo',\n",
       " 'degl',\n",
       " 'degli',\n",
       " 'dei',\n",
       " 'del',\n",
       " 'dell',\n",
       " 'della',\n",
       " 'delle',\n",
       " 'dello',\n",
       " 'dentro',\n",
       " 'di',\n",
       " 'dov',\n",
       " 'dove',\n",
       " 'e',\n",
       " 'ebbe',\n",
       " 'ebbero',\n",
       " 'ebbi',\n",
       " 'ecco',\n",
       " 'ed',\n",
       " 'era',\n",
       " 'erano',\n",
       " 'eravamo',\n",
       " 'eravate',\n",
       " 'eri',\n",
       " 'ero',\n",
       " 'essendo',\n",
       " 'faccia',\n",
       " 'facciamo',\n",
       " 'facciano',\n",
       " 'facciate',\n",
       " 'faccio',\n",
       " 'facemmo',\n",
       " 'facendo',\n",
       " 'facesse',\n",
       " 'facessero',\n",
       " 'facessi',\n",
       " 'facessimo',\n",
       " 'faceste',\n",
       " 'facesti',\n",
       " 'faceva',\n",
       " 'facevamo',\n",
       " 'facevano',\n",
       " 'facevate',\n",
       " 'facevi',\n",
       " 'facevo',\n",
       " 'fai',\n",
       " 'fanno',\n",
       " 'farai',\n",
       " 'faranno',\n",
       " 'fare',\n",
       " 'farebbe',\n",
       " 'farebbero',\n",
       " 'farei',\n",
       " 'faremmo',\n",
       " 'faremo',\n",
       " 'fareste',\n",
       " 'faresti',\n",
       " 'farete',\n",
       " 'farà',\n",
       " 'farò',\n",
       " 'fece',\n",
       " 'fecero',\n",
       " 'feci',\n",
       " 'fino',\n",
       " 'fosse',\n",
       " 'fossero',\n",
       " 'fossi',\n",
       " 'fossimo',\n",
       " 'foste',\n",
       " 'fosti',\n",
       " 'fra',\n",
       " 'fu',\n",
       " 'fui',\n",
       " 'fummo',\n",
       " 'furono',\n",
       " 'giù',\n",
       " 'gli',\n",
       " 'ha',\n",
       " 'hai',\n",
       " 'hanno',\n",
       " 'ho',\n",
       " 'i',\n",
       " 'il',\n",
       " 'in',\n",
       " 'io',\n",
       " 'l',\n",
       " 'la',\n",
       " 'le',\n",
       " 'lei',\n",
       " 'li',\n",
       " 'lo',\n",
       " 'loro',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mi',\n",
       " 'mia',\n",
       " 'mie',\n",
       " 'miei',\n",
       " 'mio',\n",
       " 'ne',\n",
       " 'negl',\n",
       " 'negli',\n",
       " 'nei',\n",
       " 'nel',\n",
       " 'nell',\n",
       " 'nella',\n",
       " 'nelle',\n",
       " 'nello',\n",
       " 'no',\n",
       " 'noi',\n",
       " 'non',\n",
       " 'nostra',\n",
       " 'nostre',\n",
       " 'nostri',\n",
       " 'nostro',\n",
       " 'o',\n",
       " 'per',\n",
       " 'perché',\n",
       " 'però',\n",
       " 'più',\n",
       " 'pochi',\n",
       " 'poco',\n",
       " 'qua',\n",
       " 'quale',\n",
       " 'quanta',\n",
       " 'quante',\n",
       " 'quanti',\n",
       " 'quanto',\n",
       " 'quasi',\n",
       " 'quella',\n",
       " 'quelle',\n",
       " 'quelli',\n",
       " 'quello',\n",
       " 'questa',\n",
       " 'queste',\n",
       " 'questi',\n",
       " 'questo',\n",
       " 'qui',\n",
       " 'quindi',\n",
       " 'sarai',\n",
       " 'saranno',\n",
       " 'sarebbe',\n",
       " 'sarebbero',\n",
       " 'sarei',\n",
       " 'saremmo',\n",
       " 'saremo',\n",
       " 'sareste',\n",
       " 'saresti',\n",
       " 'sarete',\n",
       " 'sarà',\n",
       " 'sarò',\n",
       " 'se',\n",
       " 'senza',\n",
       " 'si',\n",
       " 'sia',\n",
       " 'siamo',\n",
       " 'siano',\n",
       " 'siate',\n",
       " 'siete',\n",
       " 'sono',\n",
       " 'sopra',\n",
       " 'sotto',\n",
       " 'sta',\n",
       " 'stai',\n",
       " 'stando',\n",
       " 'stanno',\n",
       " 'starai',\n",
       " 'staranno',\n",
       " 'stare',\n",
       " 'starebbe',\n",
       " 'starebbero',\n",
       " 'starei',\n",
       " 'staremmo',\n",
       " 'staremo',\n",
       " 'stareste',\n",
       " 'staresti',\n",
       " 'starete',\n",
       " 'starà',\n",
       " 'starò',\n",
       " 'stava',\n",
       " 'stavamo',\n",
       " 'stavano',\n",
       " 'stavate',\n",
       " 'stavi',\n",
       " 'stavo',\n",
       " 'stemmo',\n",
       " 'stesse',\n",
       " 'stessero',\n",
       " 'stessi',\n",
       " 'stessimo',\n",
       " 'stesso',\n",
       " 'steste',\n",
       " 'stesti',\n",
       " 'stette',\n",
       " 'stettero',\n",
       " 'stetti',\n",
       " 'stia',\n",
       " 'stiamo',\n",
       " 'stiano',\n",
       " 'stiate',\n",
       " 'sto',\n",
       " 'su',\n",
       " 'sua',\n",
       " 'sue',\n",
       " 'sugl',\n",
       " 'sugli',\n",
       " 'sui',\n",
       " 'sul',\n",
       " 'sull',\n",
       " 'sulla',\n",
       " 'sulle',\n",
       " 'sullo',\n",
       " 'suo',\n",
       " 'suoi',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tra',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tue',\n",
       " 'tuo',\n",
       " 'tuoi',\n",
       " 'tutti',\n",
       " 'tutto',\n",
       " 'un',\n",
       " 'una',\n",
       " 'uno',\n",
       " 'vai',\n",
       " 'vi',\n",
       " 'voi',\n",
       " 'vostra',\n",
       " 'vostre',\n",
       " 'vostri',\n",
       " 'vostro',\n",
       " 'è',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " \"how's\",\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"let's\",\n",
       " 'me',\n",
       " 'more',\n",
       " 'most',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'same',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'very',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'when',\n",
       " \"when's\",\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " \"who's\",\n",
       " 'whom',\n",
       " 'why',\n",
       " \"why's\",\n",
       " 'with',\n",
       " \"won't\",\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ci' in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['criks', '16', '9', 'cv', 'ai']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww = ['criks', 'crjis3','cr456is', '45crist','1v','f4','16','l','9','5ffff56566778','cv', 'ai']\n",
    "\n",
    "falseIfDigit = lambda word: not bool((re.match('^(?=.*[0-9])', str(word))))\n",
    "\n",
    "[w for w in ww if (falseIfDigit(w) or w.isdigit()) and (len(w) > 1 or w.isdigit()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function returns FALSE if there is a digit in the string (i.e '4mmm', 'm44m', 'llp4')\n",
    "#falseIfDigit = lambda word: not bool((re.match('^(?=.*[0-9])', str(word))))\n",
    "\n",
    "def get_vocab():\n",
    "    #allWords = []\n",
    "    vocab = set()\n",
    "    for text in train_df['sentence'].values:\n",
    "        words = text.split(' ')\n",
    "        # remove digits\n",
    "        words_only = [w for w in words if not w.isdigit()]\n",
    "        # exclude words shorter than 2, but not numbers. exclude words with numbers inside, i.e. '3cris', 'c45ris', 'cris23'\n",
    "        #words_ = [w for w in words_only if (falseIfDigit(w) or w.isdigit()) and (len(w) > 1 or w.isdigit()) ]\n",
    "        words_ = [w for w in words_only if len(w) > 0 ]\n",
    "        #words_ = words_only\n",
    "        #allWords = allWords + words_\n",
    "        word_set = set(words_)\n",
    "        vocab.update(word_set)\n",
    "    \n",
    "    #vocab.remove('')\n",
    "    return list(vocab)#, allWords\n",
    "\n",
    "def get_all_words():\n",
    "    allWords = []\n",
    "    cnt = 0\n",
    "    for text in train_df['sentence'].values:\n",
    "        words = text.split(' ')\n",
    "        # remove digits\n",
    "        words_only = [w for w in words if not w.isdigit()]\n",
    "        # exclude words shorter than 2, but not numbers. exclude words with numbers inside, i.e. '3cris', 'c45ris', 'cris23'\n",
    "        #words_ = [w for w in words_only if (falseIfDigit(w) or w.isdigit()) and (len(w) > 1 or w.isdigit()) ]\n",
    "        words_ = [w for w in words_only if len(w) > 0 ]\n",
    "        #words_ = words_only\n",
    "        allWords = allWords + words_\n",
    "        #word_set = set(words_)\n",
    "        cnt += 1\n",
    "        if cnt%10000==0:\n",
    "            print('-----------', cnt)\n",
    "    \n",
    "    return allWords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 64006\n",
      "CPU times: user 820 ms, sys: 2 µs, total: 820 ms\n",
      "Wall time: 820 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = get_vocab()\n",
    "print('--------------------', len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ci' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 10000\n",
      "----------- 20000\n",
      "----------- 30000\n",
      "----------- 40000\n",
      "----------- 50000\n",
      "----------- 60000\n",
      "----------- 70000\n",
      "----------- 80000\n",
      "----------- 90000\n",
      "----------- 100000\n",
      "----------- 110000\n",
      "----------- 120000\n",
      "----------- 130000\n",
      "----------- 140000\n",
      "----------- 150000\n",
      "----------- 160000\n",
      "----------- 170000\n",
      "-------------------- 1522514\n",
      "CPU times: user 44min 42s, sys: 596 ms, total: 44min 43s\n",
      "Wall time: 44min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "allWords = get_all_words()\n",
    "print('--------------------', len(allWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt_allWords = Counter(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words_sorted_by_appearence = sorted(cnt_allWords.items(), key=lambda kv: len(vocab) - kv[1])\n",
    "#vocab_words_sorted_by_appearence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words_sorted_by_appearence_list = [word[0] for word in vocab_words_sorted_by_appearence]\n",
    "#vocab_words_sorted_by_appearence_list, len(vocab_words_sorted_by_appearence_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ci' in vocab_words_sorted_by_appearence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64006\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = False\n",
    "REDUCED_SIZE_VOC = True\n",
    "SIZE_VOC = 25000\n",
    "\n",
    "vocab = vocab_words_sorted_by_appearence_list\n",
    "\n",
    "if STOP_WORDS:\n",
    "    vocab = [w for w in vocab if w not in stop_words]\n",
    "    words_and_frequence = [ (word, freq) for (word, freq) in vocab_words_sorted_by_appearence if word not in stop_words]\n",
    "\n",
    "print(len(vocab))\n",
    "if REDUCED_SIZE_VOC:\n",
    "    vocab = vocab[0:SIZE_VOC]\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ci' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'abbiamo' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roll',\n",
       " 'mantiene',\n",
       " 'addison',\n",
       " 'pugnalato',\n",
       " 'consta',\n",
       " 'vivevo',\n",
       " 'restringe',\n",
       " 'dilettanti',\n",
       " 'caramelle',\n",
       " 'primitivo',\n",
       " 'discarica',\n",
       " 'urla',\n",
       " 'dormiva',\n",
       " 'bè',\n",
       " 'maialino',\n",
       " 'accorgano',\n",
       " 'idrogeno',\n",
       " 'sostituiti',\n",
       " 'travestita',\n",
       " 'lavoravano']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Mixed Vocabulary\n",
    "###### half of most frequent words, half of random selection among all the words (uniform distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['settimana',\n",
       " 'attività',\n",
       " 'favore',\n",
       " 'piacere',\n",
       " 'appuntamento',\n",
       " 'calendario',\n",
       " 'prossimo',\n",
       " 'evento',\n",
       " 'condividere',\n",
       " 'nome']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From 0 to boundary_point: words selected by their frequency (the most frequent words)\n",
    "# From boundary_point to len(voc): words random selected\n",
    "boundary_point = 19000\n",
    "\n",
    "def random_selection_from_vocab(vocabulary, start):\n",
    "    length_voc = len(vocabulary)\n",
    "    vocab = np.array(vocabulary)\n",
    "    indxs = np.random.choice(range(start, length_voc), length_voc - start, replace=False)\n",
    "    return list(vocab[indxs])\n",
    "\n",
    "vocab = vocab[0:boundary_point] + random_selection_from_vocab(vocab, boundary_point)\n",
    "vocab[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words_and_frequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- False\n",
      "----------------- True\n"
     ]
    }
   ],
   "source": [
    "# Check if a word is in VOC or STOP_WORDS\n",
    "ww = 'sei'\n",
    "print('-----------------', ww in stop_words)\n",
    "print('-----------------', ww in vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_WORD = '#=KS=#'\n",
    "\n",
    "PATH_VOC = os.path.join(ROOT_PATH, 'datasets/sei_6/vocab_25k_swin_v2.tsv')\n",
    "with open(PATH_VOC , 'w') as file:\n",
    "#with open('/home/asr/Data/classif_task/jsgf_data/vocab_list.tsv', 'w') as file:\n",
    "    file.write(\"{}\\n\".format(PAD_WORD))\n",
    "    for word in vocab:\n",
    "        file.write(\"{}\\n\".format(word))\n",
    "        \n",
    "PATH_WORDS = os.path.join(ROOT_PATH, 'datasets/sei_6/n_words_25k_swin_v2.tsv')        \n",
    "with open(PATH_WORDS, 'w') as file:\n",
    "#with open('/home/asr/Data/classif_task/jsgf_data/n_words.tsv', 'w') as file:\n",
    "    file.write(str(len(vocab)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw = True ---- > swout\n",
    "# sw = False ---- > swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
