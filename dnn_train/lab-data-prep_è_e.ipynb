{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asr/tensorflow-cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/asr/tensorflow-cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH='/home/asr/prj_SlotTagger/dnn_train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E/è Ambiguity Problem: VERB or CONJ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus_six_num: 599901, Corpus_six_ver: 599997, Ratio: 1.000\n",
      "CPU times: user 276 ms, sys: 18 ms, total: 294 ms\n",
      "Wall time: 451 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_lines_num  = sum(1 for line in open(ROOT_PATH + '/amb_raw/è_e_600k/cong/e_cong_max10_v1'))\n",
    "num_lines_ver  = sum(1 for line in open(ROOT_PATH + '/amb_raw/è_e_600k/ver/è_verb_max10_v3'))\n",
    "\n",
    "ratio = num_lines_ver  / float(num_lines_num) \n",
    "print('Corpus_six_num: %d, Corpus_six_ver: %d, Ratio: %0.3f' %(num_lines_num, num_lines_ver, ratio)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  599901 /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e_600k/cong/e_cong_max10_v1\n",
      "    5000 /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e_600k/cong/e_nepero5k\n",
      "     349 /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e_600k/cong/trenup_uniform_v1_e\n",
      "  605250 total\n"
     ]
    }
   ],
   "source": [
    "! wc -l /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e_600k/cong/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       5 /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e_600k/ver/additinal_è\n",
      "   25000 /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e_600k/ver/c_è_without_e_è_25k\n",
      "   25000 /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e_600k/ver/comeè_without_e_è_25k\n",
      "   25000 /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e_600k/ver/doveè_without_e_è_25k\n",
      "    4578 /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e_600k/ver/quantoè_without_e_è\n",
      "    1000 /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e_600k/ver/trenup_uniform_v1_è\n",
      "  599997 /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e_600k/ver/è_verb_max10_v3\n",
      "  680580 total\n"
     ]
    }
   ],
   "source": [
    "! wc -l /home/asr/prj_SlotTagger/dnn_train/amb_raw/è_e_600k/ver/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Data-Frame (Balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad hoc sentences\n",
    "\n",
    "months = ['gennaio', 'febbraio', 'marzo', 'aprile', 'maggio', 'giugno', 'luglio', 'agosto','settembre', \n",
    "          'ottobre','novembre','dicembre']\n",
    "mdays = ['il primo', 'il due', 'il tre', 'il quarto', 'il cinque',\n",
    "        'il sei', 'il sette', 'l otto', 'il nove', 'il dieci', 'l undici', 'il dodoci',\n",
    "        'il tredici', 'il quattordici', 'il quindici', 'il sedici', 'il diciasette', 'il diciotto',\n",
    "        'il diciannove', 'il venti', 'il ventuno', 'il ventun', 'il ventidue', 'il ventitre', 'il ventiquattro',\n",
    "        'il venticinque', 'il ventisei', 'il ventisette', 'il ventotto', 'il ventinove', \n",
    "         'il trenta', 'il trentuno', 'il trentun']\n",
    "\n",
    "è_verb_adhoc = []\n",
    "\n",
    "alphabet = 'qwertyuiopasdfghjklzxcvbnmèòà'\n",
    "alphabet = [x for x in alphabet]\n",
    "ww = lambda x: ''.join([random.choice(alphabet) for i in range(x)])\n",
    "\n",
    "for i in range(300):\n",
    "    length = [3,4,5,6,7,8,9]\n",
    "    length = random.choice(length)\n",
    "    period = ['', '', '', '', '', '', '', 'bixby', 'ehi bixby', 'ciao bixby']\n",
    "    dimmi = ['sai', 'dimmi', 'conosci', '', '', '', '', '', '', '', '']\n",
    "    s1 = '%s %s che differenza c è tra %s' %(random.choice(period), random.choice(dimmi),  ww(length))\n",
    "    #print(s1)\n",
    "    è_verb_adhoc.append(s1)\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    bixby = ['', '', '', '', '', '', '', 'bixby', 'ehi bixby', 'ciao bixby']\n",
    "    period = ['', '', '', '', '','del mese', 'dell anno', 'della settimana']\n",
    "    s1 = '%s che giorno %s è %s %s' %(random.choice(bixby), random.choice(period), \n",
    "                                         random.choice(mdays), random.choice(months))\n",
    "    è_verb_adhoc.append(s1)\n",
    "  \n",
    "santi = open(\"./amb_raw/name_entities/santi.txt\", \"r\")\n",
    "santi = [ line.rstrip().lower() for line in santi ]\n",
    "\n",
    "for i in range(100):\n",
    "    bixby = ['', '', '', '', '', '', '', 'bixby', 'ehi bixby', 'ciao bixby']\n",
    "    period = ['', '', '', '', '','del mese', 'dell anno', 'della settimana']\n",
    "    rnd = np.random.uniform()\n",
    "    if rnd < .9:\n",
    "        s1 = '%s che giorno %s è sant %s' %(random.choice(bixby), random.choice(period), \n",
    "                                         random.choice(santi))\n",
    "    else:\n",
    "        s1 = '%s in che giorno %s è sant %s' %(random.choice(bixby), random.choice(period), \n",
    "                                         random.choice(santi))\n",
    "    #print(s1)\n",
    "    è_verb_adhoc.append(s1)\n",
    "    \n",
    "cities = open(\"./amb_raw/name_entities/cities_complete.txt\", \"r\")\n",
    "cities  = [ line.rstrip().lower() for line in cities ]\n",
    "\n",
    "for i in range(200):\n",
    "    bixby = ['', '', '', '', '', '', '', 'bixby', 'ehi bixby', 'ciao bixby']\n",
    "    rnd = np.random.uniform()\n",
    "    if rnd < .5:\n",
    "        s1 = '%s in che provincia è %s' %(random.choice(bixby), random.choice(cities))\n",
    "    else:\n",
    "        length = [5,6,7,8,9]\n",
    "        length = random.choice(length)\n",
    "        s1 = '%s in che provincia è %s' %(random.choice(bixby), ww(length))\n",
    "    #print(s1)\n",
    "    è_verb_adhoc.append(s1)\n",
    "    \n",
    "for i in range(500):\n",
    "    #length = [3,4,5,6,7,8,9]\n",
    "    #length = random.choice(length)\n",
    "    prev = ['previsto bello', 'previsto sole', 'previsto soleggiato', 'previsto nuvoloso', 'prevista pioggia', 'prevista pioggia',\n",
    "               'prevista neve','previsto brutto']\n",
    "    time = ['oggi', 'domani', 'per domani','nel weekend', 'sabato', 'domenica', 'lunedì', 'martedì', 'mercoledì', 'giovedì', 'venerdì',\n",
    "           'in mattinata', 'nel pomeriggio', 'domani mattina', 'domani pomeriggio', 'domani sera']\n",
    "    s1 = 'è %s %s a %s' %(random.choice(prev), random.choice(time), random.choice(cities))\n",
    "    #print(s1)\n",
    "    è_verb_adhoc.append(s1)\n",
    "\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------    \n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "e_cong_adhoc =  []\n",
    "\n",
    "for i in range(100):\n",
    "    length = [3,4,4,5,5,6,7,8,9]\n",
    "    length = random.choice(length)\n",
    "    bbb = ['', '', '', '', '', '', '', 'bixby', 'ehi bixby', 'ciao bixby', 'in']\n",
    "    sss = ['a che ora', 'dove', 'dove andiamo','in che ora', 'quando', 'quanto costa', 'quanti siamo', 'come', 'come andiamo']\n",
    "    s1 = '%s che giorno e %s' %(random.choice(bbb),  random.choice(sss))\n",
    "    #print(s1)\n",
    "    e_cong_adhoc.append(s1)\n",
    "\n",
    "for i in range(300):\n",
    "    length = [3,4,4,5,5,6,7,8,9]\n",
    "    length = random.choice(length)\n",
    "    period = ['', '', '', '', '', '', '', 'bixby', 'ehi bixby', 'ciao bixby']\n",
    "    dimmi = ['sai', 'dimmi', 'conosci', '', '', '', '', '', '', '', '']\n",
    "    s1 = 'tra %s e %s' %( ww(length),  ww(length))\n",
    "    #print(s1)\n",
    "    e_cong_adhoc.append(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è_adhoc_written\n",
    "\n",
    "with open(\"./adhocsents/è_adhoc_written.txt\", 'w') as f:\n",
    "    for item in è_adhoc_written:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open(\"./adhocsents/e_adhoc_written.txt\", 'w') as f:\n",
    "    for item in e_adhoc_written:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "è_adhoc_written = open(\"./adhocsents/è_adhoc_written.txt\", \"r\")\n",
    "è_adhoc_written = [ line.rstrip().lower() for line in è_adhoc_written ]\n",
    "\n",
    "e_adhoc_written = open(\"./adhocsents/e_adhoc_written.txt\", \"r\")\n",
    "e_adhoc_written = [ line.rstrip().lower() for line in e_adhoc_written]\n",
    "\n",
    "\n",
    "è_verb_adhoc = open(\"./adhocsents/è_adhoc.txt\", \"r\")\n",
    "è_verb_adhoc  = [ line.rstrip().lower() for line in è_verb_adhoc]\n",
    "\n",
    "e_cong_adhoc = open(\"./adhocsents/e_adhoc.txt\", \"r\")\n",
    "e_cong_adhoc  = [ line.rstrip().lower() for line in e_cong_adhoc]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "è_adhoc = è_adhoc_written + è_verb_adhoc\n",
    "e_adhoc = e_adhoc_written + e_cong_adhoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2396, 600)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(è_adhoc), len(e_adhoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory_1, directory_2):\n",
    "    # NOTE: Put in directory_2 the largest corpus\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"class\"] = []\n",
    "    l1 = 0\n",
    "    # Append ad-hoc sentences\n",
    "    for s in è_adhoc:\n",
    "        s = s.strip()\n",
    "        data[\"sentence\"].append(s)\n",
    "        data[\"class\"].append(1)\n",
    "        \n",
    "        \n",
    "    for s in e_adhoc:\n",
    "        s = s.strip()\n",
    "        data[\"sentence\"].append(s)\n",
    "        data[\"class\"].append(0)\n",
    "        \n",
    "    for file_path in os.listdir(directory_1):\n",
    "        with tf.gfile.GFile(os.path.join(directory_1 , file_path), \"rb\") as f:\n",
    "                # strip() removes white spaces before and after the string\n",
    "                # decode() converst a byte object ('b) in a python3 string\n",
    "                list_of_sentences = [s.strip().decode() for s in f.readlines()]\n",
    "                num_rows_1 = len(list_of_sentences)\n",
    "                for i in range(num_rows_1):\n",
    "                    data[\"sentence\"].append(list_of_sentences[i])\n",
    "                    data[\"class\"].append(1)\n",
    "    \n",
    "    for file_path in os.listdir(directory_2):\n",
    "        with tf.gfile.GFile(os.path.join(directory_2 , file_path), \"rb\") as f:\n",
    "                # strip() removes white spaces before and after the string\n",
    "                # decode() converst a byte object ('b) in a python3 string\n",
    "                list_of_sentences = [s.strip().decode() for s in f.readlines() if np.random.random() <= ratio]\n",
    "                num_rows_1 = len(list_of_sentences)\n",
    "                for i in range(num_rows_1):\n",
    "                    data[\"sentence\"].append(list_of_sentences[i])\n",
    "                    data[\"class\"].append(0)\n",
    "\n",
    "    return pd.DataFrame.from_dict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.2 s, sys: 38.1 ms, total: 4.24 s\n",
      "Wall time: 4.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "directory_1 = ROOT_PATH + '/amb_raw/è_e_600k/ver/'\n",
    "directory_2 = ROOT_PATH + '/amb_raw/è_e_600k/cong/'\n",
    "\n",
    "dataset_df = load_dataset(directory_1, directory_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>682712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence\n",
       "class          \n",
       "0        605731\n",
       "1        682712"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Balanced\n",
    "dataset_df.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "di quale segno zodiacale è chi nasce a gennaio\n",
      "treni frecciabianca per la prossima settimana alle sette e quarantacinque\n",
      "di quale segno zodiacale è chi nasce a febbraio\n",
      "treni freccia per alle cinque e dieci il due novembre\n",
      "di quale segno zodiacale è chi nasce a marzo\n",
      "treni per alle diciotto e cinque domani\n",
      "di quale segno zodiacale è chi nasce a aprile\n",
      "treni diretti a alle dieci e dieci domani\n",
      "ehi bixby di quale segno zodiacale è chi nasce a maggio\n",
      "treni per alle ventidue e venti dopodomani\n",
      "ehi bixby di quale segno zodiacale è chi nasce a giugno\n",
      "treni per non prima delle dieci e dieci domani\n"
     ]
    }
   ],
   "source": [
    "# Print some samples\n",
    "for i in range(6):\n",
    "    print(dataset_df.iloc[i]['sentence'])\n",
    "    print(dataset_df.iloc[-i -1]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>di quale segno zodiacale è chi nasce a gennaio</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>di quale segno zodiacale è chi nasce a febbraio</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>di quale segno zodiacale è chi nasce a marzo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>di quale segno zodiacale è chi nasce a aprile</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ehi bixby di quale segno zodiacale è chi nasce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  class\n",
       "0     di quale segno zodiacale è chi nasce a gennaio      1\n",
       "1    di quale segno zodiacale è chi nasce a febbraio      1\n",
       "2       di quale segno zodiacale è chi nasce a marzo      1\n",
       "3      di quale segno zodiacale è chi nasce a aprile      1\n",
       "4  ehi bixby di quale segno zodiacale è chi nasce...      1"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1288438</th>\n",
       "      <td>treni per alle ventidue e venti dopodomani</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288439</th>\n",
       "      <td>treni diretti a alle dieci e dieci domani</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288440</th>\n",
       "      <td>treni per alle diciotto e cinque domani</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288441</th>\n",
       "      <td>treni freccia per alle cinque e dieci il due n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288442</th>\n",
       "      <td>treni frecciabianca per la prossima settimana ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  sentence  class\n",
       "1288438         treni per alle ventidue e venti dopodomani      0\n",
       "1288439          treni diretti a alle dieci e dieci domani      0\n",
       "1288440            treni per alle diciotto e cinque domani      0\n",
       "1288441  treni freccia per alle cinque e dieci il due n...      0\n",
       "1288442  treni frecciabianca per la prossima settimana ...      0"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    6.92936\n",
       "class       1.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting number of words and mean\n",
    "dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    20\n",
       "class        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length sentence\n",
    "dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    2.108677\n",
       "class       0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length sentence\n",
    "dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of the lengths\n",
    "%matplotlib inline\n",
    "\n",
    "length_sentence = dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1)\n",
    "plt.hist(length_sentence['sentence'],bins=range(50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c è un'emergenza a casa sua</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>orecchini e maleducazione</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comunque una volta compilato è la stessa cosa no?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perchè questa è la grecia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>questa è stata scritta da un certo alonso bilac</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>se lo è non è delle migliori</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chi è l'albert einstein di eureka?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e infatti devo fare il dolce per stasera</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tu ti metti li' e aspetti</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fortunatamente è perché sei gay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  class\n",
       "0                        c è un'emergenza a casa sua      1\n",
       "1                          orecchini e maleducazione      0\n",
       "2  comunque una volta compilato è la stessa cosa no?      1\n",
       "3                          perchè questa è la grecia      1\n",
       "4    questa è stata scritta da un certo alonso bilac      1\n",
       "5                       se lo è non è delle migliori      1\n",
       "6                 chi è l'albert einstein di eureka?      1\n",
       "7           e infatti devo fare il dolce per stasera      0\n",
       "8                          tu ti metti li' e aspetti      0\n",
       "9                    fortunatamente è perché sei gay      1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "index = [i for i in range(dataset_df.shape[0])]\n",
    "random.shuffle(index)\n",
    "dataset = dataset_df.set_index([index]).sort_index()\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove some punctuation and white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c è un emergenza a casa sua</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>orecchini e maleducazione</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comunque una volta compilato è la stessa cosa no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perchè questa è la grecia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>questa è stata scritta da un certo alonso bilac</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>se lo è non è delle migliori</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chi è l albert einstein di eureka</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e infatti devo fare il dolce per stasera</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tu ti metti li e aspetti</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fortunatamente è perché sei gay</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence  class\n",
       "0                       c è un emergenza a casa sua      1\n",
       "1                         orecchini e maleducazione      0\n",
       "2  comunque una volta compilato è la stessa cosa no      1\n",
       "3                         perchè questa è la grecia      1\n",
       "4   questa è stata scritta da un certo alonso bilac      1\n",
       "5                      se lo è non è delle migliori      1\n",
       "6                 chi è l albert einstein di eureka      1\n",
       "7          e infatti devo fare il dolce per stasera      0\n",
       "8                          tu ti metti li e aspetti      0\n",
       "9                   fortunatamente è perché sei gay      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude1 = ['\\t', '\"', '?', '!'] # list\n",
    "exclude2 = [\"'\", \"  \", \"   \", \"    \", \"     \"] # list\n",
    "\n",
    "def clean_text(text):\n",
    "    for c in exclude1:\n",
    "        text=text.replace(c,'')\n",
    "    for c in exclude2:\n",
    "        text=text.replace(c, \" \")\n",
    "    return text.lower().strip()\n",
    "\n",
    "sentence_processed = list(map(lambda text: clean_text(text), dataset['sentence'].values))\n",
    "\n",
    "dataset['sentence'] = sentence_processed\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c è un emergenza a casa sua\n",
      "la mia dieta è un po diversa\n",
      "orecchini e maleducazione\n",
      "dadá è commosso\n",
      "comunque una volta compilato è la stessa cosa no\n",
      "accidenti è vero hai ragione\n",
      "perchè questa è la grecia\n",
      "stavamo ascoltando tutti e due\n",
      "questa è stata scritta da un certo alonso bilac\n",
      "dai loro una possibilita e io la daro a te\n",
      "se lo è non è delle migliori\n",
      "la nostra casa è ridotta in cenere\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(dataset.iloc[i]['sentence'])\n",
    "    print(dataset.iloc[-i -1]['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitute ambiguities with a placeholder (tannutuva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c tannutuva un emergenza a casa sua</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>orecchini tannutuva maleducazione</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comunque una volta compilato tannutuva la stes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perchè questa tannutuva la grecia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>questa tannutuva stata scritta da un certo alo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  class\n",
       "0                c tannutuva un emergenza a casa sua      1\n",
       "1                  orecchini tannutuva maleducazione      0\n",
       "2  comunque una volta compilato tannutuva la stes...      1\n",
       "3                  perchè questa tannutuva la grecia      1\n",
       "4  questa tannutuva stata scritta da un certo alo...      1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toSubstitute = ['e', 'è'] # list\n",
    "\n",
    "# def substitute_amniguity_old(text, placeholder):\n",
    "#     for c in toSubstitute:\n",
    "#         text=text.replace(c,placeholder)\n",
    "#     return text.lower().strip()\n",
    "\n",
    "def substitute_amniguity(text, placeholder):\n",
    "    for c in toSubstitute:\n",
    "        text=re.sub(r\"\\b%s\\b\" %(c),placeholder, text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "placeholder = 'tannutuva'\n",
    "sentence_processed = list(map(lambda text: substitute_amniguity(text,placeholder), dataset['sentence'].values))\n",
    "\n",
    "dataset['sentence'] = sentence_processed\n",
    "\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split for Tagger Classifier (Train, Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Set size: 1159598\n",
      "Validation-Set size: 128845\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splitter =  model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=19850610)\n",
    "\n",
    "splits = list(splitter.split(X=dataset['sentence'], y=dataset['class']))\n",
    "train_index = splits[0][0]\n",
    "valid_index = splits[0][1]\n",
    "\n",
    "train_df = dataset.loc[train_index,:]\n",
    "print('Training-Set size: %d' %len(train_df))\n",
    "\n",
    "valid_df = dataset.loc[valid_index,:]\n",
    "print('Validation-Set size: %d' %len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "1    614440\n",
      "0    545158\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 47.01\n",
      "class 1 %: 52.99\n",
      "\n",
      "Validation Set\n",
      "1    68272\n",
      "0    60573\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 47.01\n",
      "class 1 %: 52.99\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "training_value_counts = train_df['class'].value_counts()\n",
    "print(training_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(training_value_counts[0]/len(train_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(training_value_counts[1]/len(train_df)*100,2)))\n",
    "print(\"\")\n",
    "print(\"Validation Set\")\n",
    "validation_value_counts = valid_df['class'].value_counts()\n",
    "print(validation_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(validation_value_counts[0]/len(valid_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(validation_value_counts[1]/len(valid_df)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(ROOT_PATH, 'datasets/è_e/train_data_v30.tsv'), header=False, index=False, sep='\\t')\n",
    "valid_df.to_csv(os.path.join(ROOT_PATH, 'datasets/è_e/valid_data_v30.tsv'), header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Vocabulary and Save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('italian') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function returns FALSE if there is a digit in the string (i.e '4mmm', 'm44m', 'llp4')\n",
    "falseIfDigit = lambda word: not bool((re.match('^(?=.*[0-9])', str(word))))\n",
    "\n",
    "def get_vocab():\n",
    "    #allWords = []\n",
    "    vocab = set()\n",
    "    for text in train_df['sentence'].values:\n",
    "        words = text.split(' ')\n",
    "        # remove digits\n",
    "        words_only = [w for w in words if not w.isdigit()]\n",
    "        # exclude words shorter than 2, but not numbers. exclude words with numbers inside, i.e. '3cris', 'c45ris', 'cris23'\n",
    "        #words_ = [w for w in words_only if (falseIfDigit(w) or w.isdigit()) and (len(w) > 2 or w.isdigit()) ]\n",
    "        words_ = [w for w in words_only if len(w) > 0 ]\n",
    "        #allWords = allWords + words_\n",
    "        word_set = set(words_)\n",
    "        vocab.update(word_set)\n",
    "    \n",
    "    return list(vocab)#, allWords\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "def get_all_words():\n",
    "    allWords = [text.split(' ') for text in train_df['sentence'].values]\n",
    "    allWords = list(chain(*allWords))\n",
    "    return allWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 173215\n",
      "CPU times: user 4.77 s, sys: 0 ns, total: 4.77 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = get_vocab()\n",
    "print('--------------------', len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 8166341\n",
      "CPU times: user 4.49 s, sys: 327 ms, total: 4.82 s\n",
      "Wall time: 7.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "allWords = get_all_words()\n",
    "print('--------------------', len(allWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt_allWords = Counter(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words_sorted_by_appearence = sorted(cnt_allWords.items(), key=lambda kv: len(vocab) - kv[1])\n",
    "#vocab_words_sorted_by_appearence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words_sorted_by_appearence_list = [word[0] for word in vocab_words_sorted_by_appearence]\n",
    "#vocab_words_sorted_by_appearence_list, len(vocab_words_sorted_by_appearence_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_words_sorted_by_appearence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175528\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = False\n",
    "REDUCED_SIZE_VOC = True\n",
    "SIZE_VOC = 15000\n",
    "\n",
    "vocab = vocab_words_sorted_by_appearence_list\n",
    "\n",
    "\n",
    "if STOP_WORDS:\n",
    "    vocab = [w for w in vocab if w not in stop_words]\n",
    "    words_and_frequence = [ (word, freq) for (word, freq) in vocab_words_sorted_by_appearence if word not in stop_words]\n",
    "\n",
    "print(len(vocab))\n",
    "if REDUCED_SIZE_VOC:\n",
    "    vocab = vocab[0:SIZE_VOC]\n",
    "    \n",
    "\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15001\n"
     ]
    }
   ],
   "source": [
    "adhoc_words = ['estinta', 'ballerina']\n",
    "\n",
    "for word in adhoc_words:\n",
    "    if word in vocab:\n",
    "        continue\n",
    "    else:\n",
    "        vocab.append(word)\n",
    "    \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_WORD = '#=KS=#'\n",
    "\n",
    "PATH_VOC = os.path.join(ROOT_PATH, 'datasets/è_e/vocab_15k_swin_v30_tmp.tsv')\n",
    "with open(PATH_VOC , 'w') as file:\n",
    "#with open('/home/asr/Data/classif_task/jsgf_data/vocab_list.tsv', 'w') as file:\n",
    "    file.write(\"{}\\n\".format(PAD_WORD))\n",
    "    for word in vocab:\n",
    "        file.write(\"{}\\n\".format(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "! awk '! /^[0-9]+$/' /home/asr/prj_SlotTagger/dnn_train/datasets/è_e/vocab_15k_swin_v30_tmp.tsv |  sed '/^$/d'  > /home/asr/prj_SlotTagger/dnn_train/datasets/è_e/vocab_15k_swin_v30.tsv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14890 /home/asr/prj_SlotTagger/dnn_train/datasets/è_e/vocab_15k_swin_v30.tsv\r\n"
     ]
    }
   ],
   "source": [
    "#! sed 's/[^a-zA-Z]//g' /home/asr/prj_SlotTagger/dnn_train/datasets/è_e/vocab_15k_swin_v13_tmp.tsv | awk '! /^[0-9]+$/' | sed '/^$/d' > /home/asr/prj_SlotTagger/dnn_train/datasets/è_e/vocab_15k_swin_v13.tsv \n",
    "! wc -l /home/asr/prj_SlotTagger/dnn_train/datasets/è_e/vocab_15k_swin_v30.tsv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_WORDS = os.path.join(ROOT_PATH, 'datasets/è_e/n_words_15k_swin_v30.tsv')        \n",
    "with open(PATH_WORDS, 'w') as file:\n",
    "#with open('/home/asr/Data/classif_task/jsgf_data/n_words.tsv', 'w') as file:\n",
    "    file.write(str(14890))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
